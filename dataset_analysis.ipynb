{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk. tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize, TweetTokenizer, MWETokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    return s\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = cleaning(tokens[i])\n",
    "        \n",
    "def frequency_count(tokens, include_stop_words = True):\n",
    "    if not include_stop_words:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    out = {}\n",
    "    for i in range(len(tokens)):\n",
    "        if not include_stop_words:\n",
    "            if tokens[i] in stop_words:\n",
    "                continue\n",
    "        if tokens[i] not in out:\n",
    "            out[tokens[i]] = 1\n",
    "        else:\n",
    "            out[tokens[i]] += 1\n",
    "    total = sum(out.values())\n",
    "    for key in out:\n",
    "        out[key] = (out[key], round(out[key]/total, 4))\n",
    "    return sorted(out.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from file\n",
    "with open(\"processed_data/Articles.txt\", \"r\") as f:\n",
    "    articles = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters from Articles is 4714727\n",
      "Number of tokens is 807702\n",
      "Number of unique tokens is 29058\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters from Articles is {len(articles)}\")\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "articles_words = tokenizer.tokenize(articles)\n",
    "print(f\"Number of tokens is {len(articles_words)}\")\n",
    "cleaning(articles_words)\n",
    "print(f\"Number of unique tokens is {len(set(articles_words))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequency words [('the', 41980), ('to', 21758), ('in', 20129), ('of', 18557), ('and', 17198), ('a', 16826), ('for', 9322), ('on', 9064), ('s', 8743), ('said', 6039)]\n",
      "Top 10 frequency words [('said', 6039), ('The', 5128), ('percent', 3318), ('Pakistan', 3229), ('year', 2880), ('first', 2366), ('strong', 2262), ('I', 2032), ('also', 1906), ('two', 1890)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top 10 frequency words {frequency_count(articles_words)[:10]}\")\n",
    "print(f\"Top 10 frequency words {frequency_count(articles_words, include_stop_words=False)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequency words [('said', (6039, 0.012)), ('The', (5128, 0.0101)), ('percent', (3318, 0.0066)), ('Pakistan', (3229, 0.0064)), ('year', (2880, 0.0057)), ('first', (2366, 0.0047)), ('strong', (2262, 0.0045)), ('I', (2032, 0.004)), ('also', (1906, 0.0038)), ('two', (1890, 0.0037)), ('last', (1859, 0.0037)), ('one', (1654, 0.0033)), ('oil', (1585, 0.0031)), ('England', (1557, 0.0031)), ('would', (1548, 0.0031)), ('million', (1415, 0.0028)), ('three', (1411, 0.0028)), ('prices', (1333, 0.0026)), ('India', (1312, 0.0026)), ('team', (1302, 0.0026)), ('match', (1293, 0.0026)), ('US', (1279, 0.0025)), ('day', (1261, 0.0025)), ('He', (1245, 0.0025)), ('send', (1238, 0.0025)), ('time', (1174, 0.0023)), ('world', (1172, 0.0023)), ('billion', (1144, 0.0023)), ('market', (1103, 0.0022)), ('It', (1101, 0.0022)), ('week', (1075, 0.0021)), ('four', (1028, 0.002)), ('made', (966, 0.0019)), ('since', (966, 0.0019)), ('uld', (961, 0.0019)), ('crude', (953, 0.0019)), ('World', (929, 0.0018)), ('month', (909, 0.0018)), ('untry', (906, 0.0018)), ('captain', (896, 0.0018))]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top 10 frequency words {frequency_count(articles_words, include_stop_words=False)[:40]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topSongsLyrics1950_2019.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from file\n",
    "with open(\"processed_data/topSongsLyrics1950_2019.txt\", \"r\") as f:\n",
    "    songs = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters from Articles is 1933433\n",
      "Number of tokens is 385749\n",
      "Number of unique tokens is 32644\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters from Articles is {len(songs)}\")\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "songs_words = tokenizer.tokenize(songs)\n",
    "print(f\"Number of tokens is {len(songs_words)}\")\n",
    "cleaning(songs_words)\n",
    "print(f\"Number of unique tokens is {len(set(songs_words))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequency words [('the', (41980, 0.052)), ('to', (21758, 0.0269)), ('in', (20129, 0.0249)), ('of', (18557, 0.023)), ('and', (17198, 0.0213)), ('a', (16826, 0.0208)), ('for', (9322, 0.0115)), ('on', (9064, 0.0112)), ('s', (8743, 0.0108)), ('said', (6039, 0.0075))]\n",
      "Top 10 frequency words [('said', (6039, 0.012)), ('The', (5128, 0.0101)), ('percent', (3318, 0.0066)), ('Pakistan', (3229, 0.0064)), ('year', (2880, 0.0057)), ('first', (2366, 0.0047)), ('strong', (2262, 0.0045)), ('I', (2032, 0.004)), ('also', (1906, 0.0038)), ('two', (1890, 0.0037))]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top 10 frequency words {frequency_count(songs_words)[:10]}\")\n",
    "print(f\"Top 10 frequency words {frequency_count(songs_words, include_stop_words=False)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequency words [('I', (11968, 0.0493)), ('The', (2647, 0.0109)), ('You', (2248, 0.0093)), ('And', (1958, 0.0081)), ('love', (1687, 0.0069)), ('like', (1665, 0.0069)), ('oh', (1550, 0.0064)), ('know', (1464, 0.006)), ('got', (1178, 0.0049)), ('It', (1174, 0.0048)), ('yeah', (1155, 0.0048)), ('get', (1147, 0.0047)), ('Oh', (1057, 0.0044)), ('baby', (961, 0.004)), ('one', (908, 0.0037)), ('We', (897, 0.0037)), ('But', (865, 0.0036)), ('go', (763, 0.0031)), ('Yeah', (689, 0.0028)), ('A', (662, 0.0027)), ('time', (654, 0.0027)), ('make', (636, 0.0026)), ('way', (633, 0.0026)), ('feat', (631, 0.0026)), ('want', (617, 0.0025)), ('say', (605, 0.0025)), ('see', (600, 0.0025)), ('That', (596, 0.0025)), ('My', (593, 0.0024)), ('So', (582, 0.0024)), ('Don', (576, 0.0024)), ('right', (575, 0.0024)), ('feel', (571, 0.0024)), ('back', (563, 0.0023)), ('never', (559, 0.0023)), ('All', (545, 0.0022)), ('let', (544, 0.0022)), ('good', (534, 0.0022)), ('gonna', (525, 0.0022)), ('She', (523, 0.0022))]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top 10 frequency words {frequency_count(songs_words, include_stop_words=False)[:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
